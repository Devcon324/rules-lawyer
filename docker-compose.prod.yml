services:
  neo4j:
    container_name: neo4j-rules-lawyer
    image: neo4j:2025.10.1-community-bullseye
    restart: always
    # Ports not exposed externally - accessed through internal network only
    expose:
      - "7474"
      - "7687"
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME}/${NEO4J_PASSWORD}
    volumes:
      - neo4j_data:/var/lib/neo4j/data              # NOT /data
      - neo4j_logs:/var/lib/neo4j/logs              # NOT /logs
      - neo4j_conf:/var/lib/neo4j/conf              # Configuration files
      - neo4j_plugins:/var/lib/neo4j/plugins        # Plugins directory
    networks:
      - backend-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:7474"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend-rules-lawyer
    restart: always
    # Port not exposed externally - accessed through nginx reverse proxy
    expose:
      - "8000"
    volumes:
      # Only mount data directory for persistent storage
      - backend_data:/app/data
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_DATABASE=${NEO4J_DATABASE}
      - NEO4J_USERNAME=${NEO4J_USERNAME}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      - NEO4J_INDEX=${NEO4J_INDEX}
      - NEO4J_NODE_LABEL=${NEO4J_NODE_LABEL}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL}
      - OLLAMA_GENERATIVE_MODEL=${OLLAMA_GENERATIVE_MODEL}
      - AUTH_USERNAME=${AUTH_USERNAME}
      - AUTH_PASSWORD=${AUTH_PASSWORD}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
    depends_on:
      neo4j:
        condition: service_healthy
      ollama:
        condition: service_started  # or service_healthy if you add healthcheck
    networks:
      - frontend-network
      - backend-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s  # Add start period for initial startup


  nginx:
    build:
      context: .
      dockerfile: ./nginx/Dockerfile.prod
    container_name: nginx-rules-lawyer
    restart: always
    ports:
      - "80:80"
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - frontend-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  ollama:
    image: ollama/ollama:0.13.0
    container_name: ollama-rules-lawyer
    restart: always
    # Port not exposed externally - accessed through internal network only
    expose:
      - "11434"
    volumes:
      - ollama:/root/.ollama
    networks:
      - backend-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 10G
        reservations:
          cpus: '1.0'
          memory: 3G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Models take time to load

  ngrok:
    image: ngrok/ngrok:3.33.1-alpine
    container_name: ngrok-rules-lawyer
    restart: always
    command:
      - "start"
      - "--all"
      - "--config"
      - "/etc/ngrok.yml"
    # ports:
    #   - "4040:4040"  # ngrok web interface, uncomment for debugging
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
    volumes:
      - ./ngrok/ngrok.yml:/etc/ngrok.yml:ro
    depends_on:
      nginx:
        condition: service_healthy
    networks:
      - frontend-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_conf:
    driver: local
  neo4j_plugins:
    driver: local
  backend_data:
    driver: local
  ollama:
    external: true

networks:
  frontend-network:
    driver: bridge
    # External access allowed (nginx needs to accept incoming traffic)
  backend-network:
    driver: bridge
    # External access allowed (backend services need internet for: Ollama model downloads, package installs, external APIs)
    # Services are still protected - only nginx has exposed ports


